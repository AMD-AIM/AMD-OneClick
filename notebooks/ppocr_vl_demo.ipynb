{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç PaddleOCR-VL Demo on AMD GPU\n",
        "\n",
        "This notebook demonstrates how to use **PaddleOCR-VL** (Vision-Language OCR) model running on AMD GPU with vLLM backend.\n",
        "\n",
        "## Features\n",
        "- **Layout Detection**: Detect document structure (text, images, tables, titles)\n",
        "- **VL Recognition**: Vision-Language model for accurate text recognition\n",
        "- **vLLM Backend**: High-performance inference on AMD GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Check vLLM Server Status\n",
        "\n",
        "First, let's verify that the vLLM server is running and ready.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def wait_for_vllm(url=\"http://localhost:8118/v1/models\", timeout=300, interval=5):\n",
        "    \"\"\"\n",
        "    Wait for vLLM server to be ready.\n",
        "    \n",
        "    Args:\n",
        "        url: vLLM models endpoint\n",
        "        timeout: Maximum wait time in seconds (default: 5 minutes)\n",
        "        interval: Check interval in seconds\n",
        "    \n",
        "    Returns:\n",
        "        True if server is ready, False if timeout\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    attempt = 0\n",
        "    \n",
        "    print(\"üîÑ Waiting for vLLM server to be ready...\")\n",
        "    print(f\"   (This may take 1-2 minutes as the model loads)\")\n",
        "    print()\n",
        "    \n",
        "    while time.time() - start_time < timeout:\n",
        "        attempt += 1\n",
        "        elapsed = int(time.time() - start_time)\n",
        "        \n",
        "        try:\n",
        "            response = requests.get(url, timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                models = response.json()\n",
        "                clear_output(wait=True)\n",
        "                print(\"‚úÖ vLLM Server is ready!\")\n",
        "                print(f\"   Time elapsed: {elapsed} seconds\")\n",
        "                print(f\"   Available models: {[m['id'] for m in models['data']]}\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            pass\n",
        "        \n",
        "        # Show progress\n",
        "        clear_output(wait=True)\n",
        "        spinner = [\"‚†ã\", \"‚†ô\", \"‚†π\", \"‚†∏\", \"‚†º\", \"‚†¥\", \"‚†¶\", \"‚†ß\", \"‚†á\", \"‚†è\"][attempt % 10]\n",
        "        print(f\"{spinner} Waiting for vLLM server... ({elapsed}s elapsed)\")\n",
        "        print(f\"   Attempt {attempt}, checking every {interval}s\")\n",
        "        print(f\"   Timeout: {timeout}s\")\n",
        "        \n",
        "        time.sleep(interval)\n",
        "    \n",
        "    clear_output(wait=True)\n",
        "    print(f\"‚ùå Timeout after {timeout} seconds\")\n",
        "    print(\"   Please check vLLM server logs: /var/log/vllm_server.log\")\n",
        "    return False\n",
        "\n",
        "# Wait for vLLM server\n",
        "wait_for_vllm()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load PaddleOCR-VL Pipeline\n",
        "\n",
        "Load the PaddleOCR-VL pipeline with vLLM backend configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/opt/PaddleX\")\n",
        "\n",
        "# Set environment variable to skip model source check\n",
        "os.environ[\"PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK\"] = \"True\"\n",
        "\n",
        "from paddlex import create_pipeline\n",
        "\n",
        "# Create the PaddleOCR-VL pipeline with vLLM backend\n",
        "pipeline = create_pipeline(pipeline=\"PaddleOCR-VL-vllm.yaml\")\n",
        "print(\"‚úÖ Pipeline loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Display Test Image\n",
        "\n",
        "Let's view the demo image before running OCR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Display the test image\n",
        "test_image = \"/opt/PaddleX/test/paddleocr_vl_demo.png\"\n",
        "print(f\"Test image: {test_image}\")\n",
        "display(Image(filename=test_image, width=600))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run OCR Inference\n",
        "\n",
        "Execute the PaddleOCR-VL pipeline on the test image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Run inference\n",
        "result = pipeline.predict(test_image)\n",
        "\n",
        "print(\"‚úÖ Inference completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. View Results\n",
        "\n",
        "Display the extracted text content from the document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process and display results\n",
        "for res in result:\n",
        "    # Get parsing results - res is a PaddleOCRVLResult (dict-like)\n",
        "    parsing_list = res.get('parsing_res_list', [])\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"üìÑ EXTRACTED CONTENT\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for i, block in enumerate(parsing_list):\n",
        "        # block is a PaddleOCRVLBlock object, access attributes directly\n",
        "        label = getattr(block, 'label', 'unknown')\n",
        "        content = getattr(block, 'content', '')\n",
        "        \n",
        "        if content:  # Only show blocks with content\n",
        "            label_emoji = {\n",
        "                'doc_title': 'üìå',\n",
        "                'paragraph_title': 'üìç',\n",
        "                'text': 'üìù',\n",
        "                'image': 'üñºÔ∏è',\n",
        "                'table': 'üìä',\n",
        "                'vision_footnote': 'üìé'\n",
        "            }.get(label, '‚Ä¢')\n",
        "            \n",
        "            print(f\"\\n{label_emoji} [{label.upper()}]\")\n",
        "            print(\"-\" * 40)\n",
        "            print(content[:500] + ('...' if len(content) > 500 else ''))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéâ Congratulations!\n",
        "\n",
        "You have successfully run PaddleOCR-VL on AMD GPU with vLLM backend.\n",
        "\n",
        "### Resources\n",
        "- [PaddleX Documentation](https://paddlepaddle.github.io/PaddleX/)\n",
        "- [PaddleOCR-VL Tutorial](https://github.com/PaddlePaddle/PaddleX)\n",
        "- [AMD ROCm](https://rocm.docs.amd.com/)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
